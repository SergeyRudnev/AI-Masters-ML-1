{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fdb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.special import logsumexp\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import numpy.testing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "cd55bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7e3fa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function_negative_inf_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "0ff94ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function_positive_inf_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ca728c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e5730f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "797fbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gradient_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3cf2e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numeric_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06098b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.losses import BinaryLogisticLoss\n",
    "from modules.linear_model import LinearModel\n",
    "from modules.utils import get_numeric_grad\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import numpy.testing as npt\n",
    "import time\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_function():\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=1.0)\n",
    "    X = np.array([\n",
    "        [1, 1, 2],\n",
    "        [1, 3, 4],\n",
    "        [1, -5, 6]\n",
    "    ])\n",
    "    y = np.array([-1, 1, 1])\n",
    "    w = np.array([1, 2, 3])\n",
    "    npt.assert_almost_equal(loss_function.func(X, y, w), 16.00008, decimal=5)\n",
    "\n",
    "def test_function_negative_inf_values():\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=0.0)\n",
    "    X = np.array([\n",
    "        [1, 10 ** 5],\n",
    "        [1, -10 ** 5],\n",
    "        [1, 10 ** 5]\n",
    "    ])\n",
    "    y = np.array([1, -1, 1])\n",
    "    w = np.array([1, 100])\n",
    "    npt.assert_almost_equal(loss_function.func(X, y, w), 0, decimal=5)\n",
    "\n",
    "def test_function_positive_inf_values():\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=0.0)\n",
    "    X = np.array([\n",
    "        [1, 10 ** 2],\n",
    "        [1, -10 ** 2],\n",
    "        [1, 10 ** 2]\n",
    "    ])\n",
    "    y = np.array([-1, 1, -1])\n",
    "    w = np.array([1, 100])\n",
    "    npt.assert_almost_equal(loss_function.func(X, y, w), 10000.333334, decimal=5)\n",
    "\n",
    "\n",
    "def test_gradient():\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=1.0)\n",
    "    X = np.array([\n",
    "        [1, 1, 2],\n",
    "        [1, 3, 4],\n",
    "        [1, -5, 6]\n",
    "    ])\n",
    "    y = np.array([-1, 1, 1])\n",
    "    w = np.array([1, 2, 3])\n",
    "    right_gradient = np.array([0.33325, 4.3335 , 6.66634])\n",
    "    npt.assert_almost_equal(loss_function.grad(X, y, w), right_gradient, decimal=5)\n",
    "\n",
    "\n",
    "def test_function_sparse():\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=1.0)\n",
    "    X = csr_matrix(np.array([\n",
    "        [1, 1, 2],\n",
    "        [1, 3, 4],\n",
    "        [1, -5, 6]\n",
    "    ]))\n",
    "    y = np.array([-1, 1, 1])\n",
    "    w = np.array([1, 2, 3])\n",
    "    npt.assert_almost_equal(loss_function.func(X, y, w),  16.000082, decimal=5)\n",
    "\n",
    "\n",
    "def test_gradient_sparse():\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=1.0)\n",
    "    X = csr_matrix(np.array([\n",
    "        [1, 1, 2],\n",
    "        [1, 3, 4],\n",
    "        [1, -5, 6]\n",
    "    ]))\n",
    "    y = np.array([-1, 1, 1])\n",
    "    w = np.array([1, 2, 3])\n",
    "    right_gradient = np. array([0.33325, 4.3335 , 6.66634])\n",
    "    npt.assert_almost_equal(loss_function.grad(X, y, w), right_gradient, decimal=5)\n",
    "\n",
    "\n",
    "def test_numeric_grad():\n",
    "    result = get_numeric_grad(lambda x: (x ** 2).sum(), np.array([1, 2, 3]), 1e-6)\n",
    "    npt.assert_almost_equal(result, np.array([2, 4, 6]), decimal=5)\n",
    "\n",
    "\n",
    "def create_simple_dataset():\n",
    "    X1 = np.random.randint(1, 4, (1000, 10))\n",
    "    X2 = np.random.randint(-4, 0, (1000, 10))\n",
    "    X = csr_matrix(np.vstack((X1, X2)))\n",
    "    y = np.array([-1] * 1000 + [1] * 1000)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def test_simple_classification_task():\n",
    "    X, y = create_simple_dataset()\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=0.1)\n",
    "    linear_model = LinearModel(\n",
    "        loss_function=loss_function,\n",
    "        batch_size=100,\n",
    "        step_alpha=1,\n",
    "        step_beta=0,\n",
    "        tolerance=1e-5,\n",
    "        max_iter=1000,\n",
    "    )\n",
    "    linear_model.fit(X, y)\n",
    "    predictions = linear_model.predict(X)\n",
    "    npt.assert_equal(predictions, y)\n",
    "\n",
    "\n",
    "def test_logging():\n",
    "    X, y = create_simple_dataset()\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=0.1)\n",
    "    linear_model = LinearModel(\n",
    "        loss_function=loss_function,\n",
    "        batch_size=None,\n",
    "        step_alpha=1,\n",
    "        step_beta=0,\n",
    "        tolerance=1e-100,\n",
    "        max_iter=5,\n",
    "    )\n",
    "    history = linear_model.fit(X, y, trace=True)\n",
    "    for key in ['time', 'func']:\n",
    "        assert key in history\n",
    "        assert len(history[key]) == 5\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"step_alpha, step_beta, answer\", [\n",
    "    (1e-1, 0.5, 0.713865),\n",
    "    (0.6, 1, 15.134696),\n",
    "    (0.6, 1.1,  1.436495),\n",
    "])\n",
    "def test_full_gd(step_alpha, step_beta, answer):\n",
    "    X = csr_matrix(np.array([\n",
    "        [1, 0, 0, 2, 5, 0.9],\n",
    "        [1, 5, 1, 3, 1, 0.1],\n",
    "        [1, 0, 0, 2, 1, 0.5],\n",
    "        [1, 5, 1, 4, 3, 0.32],\n",
    "        [1, 0, 2, 3, 2, 0.1],\n",
    "        [1, 5, 2, 5, 4, 0.10],\n",
    "        [1, 0, 0, 6, 6, 0.28],\n",
    "        [1, 5, 1, 3, 2, 0.7],\n",
    "    ]))\n",
    "\n",
    "    y = np.array([-1, -1, -1, -1, 1, 1, 1, 1])\n",
    "    w_0 = np.array([0.5, 0.1, 0.3, 0.5, 0.3, 0.5])\n",
    "\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=5)\n",
    "    lm = LinearModel(\n",
    "        loss_function=loss_function,\n",
    "        step_alpha=step_alpha,\n",
    "        step_beta=step_beta,\n",
    "        tolerance=1e-5,\n",
    "        max_iter=5,\n",
    "    )\n",
    "    lm.fit(X, y, w_0=w_0)\n",
    "    npt.assert_almost_equal(lm.loss_function.func(X, y, lm.get_weights()), answer, decimal=5)\n",
    "\n",
    "\n",
    "def test_real_sparse_problem():\n",
    "    data = np.array([1, 1, 1, 1, 1])\n",
    "    row_ind = np.array([0, 10 ** 3, 10 ** 4, 10 ** 5, 10 ** 6])\n",
    "    col_ind = np.array([0, 10 ** 4, 10 ** 5, 10 ** 5, 10 ** 6])\n",
    "    X = csr_matrix((data, (row_ind, col_ind)))\n",
    "    X = csr_matrix(hstack([csr_matrix(np.ones((X.shape[0], 1))), X]))\n",
    "    y = np.array([1] * (10 ** 6 + 1))\n",
    "    y[:5 * 10 ** 5] = -1\n",
    "    w = np.ones(10 ** 6 + 2)\n",
    "    loss_function = BinaryLogisticLoss(l2_coef=5)\n",
    "    start = time.time()\n",
    "    loss_function.func(X, y, w)\n",
    "    finish = time.time() - start\n",
    "    assert finish < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9a3d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.001\n",
    "max_iter = 1000\n",
    "step_alpha = 1\n",
    "step_beta = 2\n",
    "batch_size = None\n",
    "trace = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5b28602",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = np.array([0.5,1,3,2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "328267a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearModel(\n",
    "        loss_function,\n",
    "        batch_size=1,\n",
    "        step_alpha=1,\n",
    "        step_beta=2, \n",
    "        tolerance=1e-5,\n",
    "        max_iter=100,\n",
    "        random_seed=153,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1be71120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2956e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49343904839051844"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_objective(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "88fceadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import time\n",
    "\n",
    "\n",
    "class LinearModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_function,\n",
    "        batch_size=None,\n",
    "        step_alpha=1,\n",
    "        step_beta=0, \n",
    "        tolerance=1e-5,\n",
    "        max_iter=1000,\n",
    "        random_seed=153,\n",
    "        w=None,\n",
    "        w_val=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_function : BaseLoss inherited instance\n",
    "            Loss function to use\n",
    "        batch_size : int\n",
    "        step_alpha : float\n",
    "        step_beta : float\n",
    "            step_alpha and step_beta define the learning rate behaviour\n",
    "        tolerance : float\n",
    "            Tolerace for stop criterio.\n",
    "        max_iter : int\n",
    "            Max amount of epoches in method.\n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.random_seed = random_seed\n",
    "        self.w = w\n",
    "\n",
    "    def fit(self, X, y, w_0=None, trace=False, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_matrix\n",
    "            2d matrix, training set.\n",
    "        y : numpy.ndarray\n",
    "            1d vector, target values.\n",
    "        w_0 : numpy.ndarray\n",
    "            1d vector in binary classification.\n",
    "            2d matrix in multiclass classification.\n",
    "            Initial approximation for SGD method.\n",
    "        trace : bool\n",
    "            If True need to calculate metrics on each iteration.\n",
    "        X_val : numpy.ndarray or scipy.sparse.csr_matrix\n",
    "            2d matrix, validation set.\n",
    "        y_val: numpy.ndarray\n",
    "            1d vector, target values for validation set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        : dict\n",
    "            Keys are 'time', 'func', 'func_val'.\n",
    "            Each key correspond to list of metric values after each training epoch.\n",
    "        \"\"\" \n",
    "        import timeit   \n",
    "        epochs_time = []; epochs_loss = []; epochs_loss_val =[]; history = {} # для сохранения истории обучения\n",
    "        if w_0 is None:  # задаём начальное значение весов, если оно не задано\n",
    "            w_0 = np.zeros(X.shape[1])\n",
    "        k = 0\n",
    "        diff = 100  # произвольное число, заведомо большее tolerance\n",
    "        w_k = w_0\n",
    "        if self.batch_size is None: # обычный градиентный спуск с убывающим темпом обучения\n",
    "            while (k < self.max_iter) & (diff > self.tolerance):\n",
    "                if trace:\n",
    "                    start_time = timeit.default_timer() \n",
    "                step_eta = self.step_alpha / ((k+1) ** self.step_beta)\n",
    "                w_k1 = w_k - step_eta * self.loss_function.grad(X, y, w_k)  # шаг обучения\n",
    "                diff = w_k1 @ w_k1.T + w_k @ w_k.T - 2 * w_k1 @ w_k.T\n",
    "                w_k = w_k1\n",
    "                k += 1               \n",
    "                if trace:\n",
    "                    epochs_time.append(timeit.default_timer() - start_time)\n",
    "                    epochs_loss.append(self.loss_function.func(X, y, w_k))\n",
    "                    if (X_val is not None) & (y_val is not None):\n",
    "                        epochs_loss_val.append(self.loss_function.func(X_val, y_val, w_k))\n",
    "        else: # стохастический градиентный спуск, сначала все признаки перемешиваются, а затем последовательно \n",
    "              # выбирается выборка размера batch_size\n",
    "            import math as m\n",
    "            number_of_steps = m.ceil(X.shape[0] / self.batch_size)\n",
    "            while (k < self.max_iter) & (diff > self.tolerance): # под max_iter понимаем количество проходов по всей обуч. выборке\n",
    "                if trace:\n",
    "                    start_time = timeit.default_timer()\n",
    "                np.random.shuffle(X)\n",
    "                step_eta = self.step_alpha / ((k+1) ** self.step_beta)\n",
    "                w_epoch_k = w_k\n",
    "                for i in range(number_of_steps):  # выполняем градиентный спуск для каждого батча \n",
    "                    w_epoch_k1 = w_epoch_k - step_eta * self.loss_function.grad(X[self.batch_size*i:self.batch_size*(i+1), :], y[self.batch_size*i:self.batch_size*(i+1)], w_epoch_k)            \n",
    "                    w_epoch_k = w_epoch_k1\n",
    "                w_k1 = w_epoch_k\n",
    "                diff = w_k1 @ w_k1.T + w_k @ w_k.T - 2 * w_k1 @ w_k.T\n",
    "                w_k = w_k1\n",
    "                k += 1 \n",
    "                if trace:\n",
    "                    epochs_time.append(timeit.default_timer() - start_time)\n",
    "                    epochs_loss.append(self.loss_function.func(X, y, w_k))\n",
    "                    if (X_val is not None) & (y_val is not None):\n",
    "                        epochs_loss_val.append(self.loss_function.func(X_val, y_val, w_k))\n",
    "        self.w = w_k\n",
    "        if trace:\n",
    "            history['time'] = epochs_time\n",
    "            history['func'] = epochs_loss\n",
    "            if (X_val is not None) & (y_val is not None):\n",
    "                 history['func_val'] = epochs_loss_val\n",
    "            return history\n",
    "        \n",
    "      \n",
    "    def predict(self, X, threshold=0):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_matrix\n",
    "            2d matrix, test set.\n",
    "        threshold : float\n",
    "            Chosen target binarization threshold.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        : numpy.ndarray\n",
    "            answers on a test set\n",
    "        \"\"\"\n",
    "        return np.sign(expit(X @ self.get_weights()) - threshold) \n",
    "        # то есть выдаём 1, если сигмойда превышает значение threshold\n",
    "        \n",
    "\n",
    "    def get_optimal_threshold(self, X, y):\n",
    "        \"\"\"\n",
    "        Get optimal target binarization threshold.\n",
    "        Balanced accuracy metric is used.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_matrix\n",
    "            2d matrix, validation set.\n",
    "        y : numpy.ndarray\n",
    "            1d vector, target values for validation set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        : float\n",
    "            Chosen threshold.\n",
    "        \"\"\"\n",
    "        if self.loss_function.is_multiclass_task:\n",
    "            raise TypeError('optimal threhold procedure is only for binary task')\n",
    "\n",
    "        weights = self.get_weights()\n",
    "        scores = X.dot(weights)\n",
    "        y_to_index = {-1: 0, 1: 1}\n",
    "\n",
    "        # for each score store real targets that correspond score\n",
    "        score_to_y = dict()\n",
    "        score_to_y[min(scores) - 1e-5] = [0, 0]\n",
    "        for one_score, one_y in zip(scores, y):\n",
    "            score_to_y.setdefault(one_score, [0, 0])\n",
    "            score_to_y[one_score][y_to_index[one_y]] += 1\n",
    "\n",
    "        # ith element of cum_sums is amount of y <= alpha\n",
    "        scores, y_counts = zip(*sorted(score_to_y.items(), key=lambda x: x[0]))\n",
    "        cum_sums = np.array(y_counts).cumsum(axis=0)\n",
    "\n",
    "        # count balanced accuracy for each threshold\n",
    "        recall_for_negative = cum_sums[:, 0] / cum_sums[-1][0]\n",
    "        recall_for_positive = 1 - cum_sums[:, 1] / cum_sums[-1][1]\n",
    "        ba_accuracy_values = 0.5 * (recall_for_positive + recall_for_negative)\n",
    "        best_score = scores[np.argmax(ba_accuracy_values)]\n",
    "        return best_score\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get model weights\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        : numpy.ndarray\n",
    "            1d vector in binary classification.\n",
    "            2d matrix in multiclass classification.\n",
    "            Initial approximation for SGD method.\n",
    "        \"\"\"\n",
    "        return self.w\n",
    "\n",
    "    def get_objective(self, X, y):\n",
    "        \"\"\"\n",
    "        Get objective.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray or scipy.sparse.csr_matrix\n",
    "            2d matrix.\n",
    "        y : numpy.ndarray\n",
    "            1d vector, target values for X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        : float\n",
    "        \"\"\"\n",
    "        return self.loss_function.func(X, y, self.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e2c794b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_grad(f, x, eps):\n",
    "\t\"\"\"\n",
    "\tFunction to calculate numeric gradient of f function in x.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tf : callable\n",
    "\tx : numpy.ndarray\n",
    "\t\t1d array, function argument\n",
    "\teps : float\n",
    "\t\tTolerance\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\t: numpy.ndarray\n",
    "\t\tNumeric gradient.\n",
    "\t\"\"\"\n",
    "\t# сделаем матрицу, столбцы которой - измененённый по одной компоненте вектор x\n",
    "\tdef numeric_grad(a):\n",
    "\t\treturn (f(a) - f(x)) / eps\n",
    "\treturn np.apply_along_axis(numeric_grad, 0,\n",
    "\t\tx.reshape(-1, 1) @ np.ones(x.shape[0]).reshape(1, -1) + eps * np.eye(x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef620308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b204f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5cf83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
