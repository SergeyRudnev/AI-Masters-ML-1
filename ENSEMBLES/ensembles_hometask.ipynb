{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли\n",
    "\n",
    "### OzonMasters, \"Машинное обучение 1\"\n",
    "\n",
    "В этом ноутбуке вам предлагается реализовать алгоритмы бустинга и бэггинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сэмплирование случайных объектов и признаков\n",
    "\n",
    "Во многих ансамблевых алгоритмах используется прием, заключающийся в обучении на случайной подвыборке объектов или на случайном подмножестве признаков.\n",
    "\n",
    "Реализуйте класс, который будет упрощать семплирование различных подмассивов данных: `BaseSampler`.\n",
    "\n",
    "В классе `BaseSampler` реализуйте метод `sample_indices` который по числу сущностей `n_objects` возращает случайную подвыборку индексов. Используйте атрибут `self.random_state`, чтобы результаты семпплирования воспроиводились. Используйте атрибут `self.bootstrap`, если нужно выбрать случайную подвыборку с возвращением.\n",
    "\n",
    "У класса `ObjectSampler` реализован метод `sample`, который возвращает случайную подвыборку объектов обучения и ответы для них.\n",
    "\n",
    "В классе `FeaturesSampler` реализован метод `sample`, который возвращает случайную подвыборку признаков для объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Бэггинг (2 балла)\n",
    "\n",
    "Суть бэггинга заключается в обучении нескольких \"слабых\" базовых моделей и объединении их в одну модель, обладающую бОльшей обобщающей способностью. Каждая базовая модель обучается на случайно выбранном подмножестве объектов и на случайно выбранном подмножестве признаков для этих объектов.\n",
    "\n",
    "Ниже вам предлагается реализовать несколько методов класса `Bagger`:\n",
    "* `fit` - обучение базовых моделей\n",
    "* `predict_proba` - вычисление вероятностей ответов.\n",
    "\n",
    "Тогда алгоритм случайный лес будет бэггингом над решающими деревьями. Реализация случайного веса представлена в классе `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Градиентный бустинг (2 балла)\n",
    "\n",
    "Бустинг последовательно обучает набор базовых моделей таким образом, что каждая следующая модель пытается исправить ошибки работы предыдущей модели. Логика того, как учитываются ошибки предыдущей модели может быть разной. В алгоритме градиентного бустинга каждая следующая модель обучается на \"невязках\" предыдущей модели, минимизируя итоговую функцию потерь. У каждого следующего алгоритма вычисляется вес $\\alpha$, с которым он входит в ансамбль. Также есть параметр скорости обучения (learning rate), который не позволяет алгоритму переобучитсья. Вес $\\alpha$ можно находить, используя одномерную оптимизацию. Можно записать процедуру обучения по шагам (будем рассматривать случай бинарной классификации c метками классов {0,1}, чтобы не усложнять жизнь):\n",
    "1. Настройка базового алгоритма $b_0$.\n",
    "    \n",
    "    Алгоритм настраиваются на $y$ с помощью функции MSE.\n",
    "    \n",
    "    \n",
    "2. Будем обозначать текущий небазовый алгоритм - $a$:\n",
    "    \n",
    "    $$a_i(x) = \\sum_{j=0}^i \\alpha_j b_j(x) $$\n",
    "    \n",
    "3. Настройка базового алгоритма $b_i$ (обычно это регрессионное дерево):\n",
    "    \n",
    "    $$b_i = \\arg \\min_b \\sum_{j=1}^l (b(x_j) + \\nabla L(a_{i-1}(x_j), y))^2,$$\n",
    "    т.е. выход очередного базового алгоритма подстраивается под антиградиент функции потерь\n",
    "    \n",
    "4. Настройка веса базового алгоритма $\\alpha_i$:\n",
    "    \n",
    "    $$\\alpha_i = \\min_{\\alpha > 0} \\sum_{j=1}^l L(a_{i-1} + \\alpha b_i(x_j), y) $$\n",
    "    \n",
    "В случае классфикации будем использовать логистическую функцию потерь. Немного упростим ее:\n",
    "\n",
    "$$L = -y\\log\\sigma(a) - (1-y)\\log(1 - \\sigma(a)) = -\\log(1 - \\sigma(a)) - y \\log \\frac{\\sigma(a)}{1 - \\sigma(a)},$$\n",
    "где $\\sigma$ - функция сигмоиды. Ответ после очередного базового алгоритма надо прогонять через сигмоиду, т.к. не гарантируется, что ответы будут лежать на [0,1] - в этом особенность базового алгоритма (который является регрессионным).\n",
    "\n",
    "Преобразуем:\n",
    "$$\\log (1 - \\sigma(a)) = \\log \\frac{1}{1 + \\exp(a)} = -\\log(1 + \\exp(a)) $$\n",
    "\n",
    "$$\\log (\\frac{\\sigma(a)}{1 - \\sigma(a)}) = \\log(\\exp(a)) = a $$\n",
    " \n",
    "Таким образом:\n",
    "\n",
    "$$L = -ya + \\log(1 + \\exp(a))$$\n",
    "\n",
    "Тогда будем вычислять градиент как:\n",
    " \n",
    "$$\\nabla L = - y + \\sigma(a)$$\n",
    "\n",
    "В классе `Booster` реализуйте методы:\n",
    "* `_fit_first_estimator` – построение первой модели (первого приближения данных);\n",
    "* `fit` – обучение алгоритма градиентного бустинга (обучение первой и последующих базовых моделей);\n",
    "* `predict` – получение предсказаний алгоритма градиентного бустинга.\n",
    "\n",
    "В классе `GradientBoostingClassifier` реализуйте методы:\n",
    "* `_fit_base_estimator` - обучение очередной базовой модели;\n",
    "* `_gradient` - расчет градиента функции ошибки;\n",
    "* `_loss` - расчет функции ошибки (для одномерно оптимизации)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты (3 балла)\n",
    "\n",
    "Скачайте датасейт для экспериментов: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "\n",
    "Колонка с ответами - RainTommorow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('weatherAUS.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m-%d')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделите признаки год/месяц/день:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = ###\n",
    "data['month'] = ###\n",
    "data['day'] = ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие года есть в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на три части (train, val и test) по временному принципу:\n",
    "    \n",
    "* train - 2007-2014\n",
    "* val - 2015\n",
    "* test - 2016-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = {\n",
    "    'train': ###,\n",
    "    'val': ###,\n",
    "    'test': ###\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь вы можете делать всевозможные преобразования признаков. \n",
    "\n",
    "Для того, чтобы получить качество, необходимое для преодоления бейзлайна, вам достаточно закодировать все категориальные признаки с помощью `LabelEncoder`, а также разумно обработать пропущенные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваш таргет - RainTommorow. Удалите его из обучающих данных, также удалите признак RISK_MM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = data['RainTomorrow']\n",
    "data.drop(['RainTomorrow', 'RISK_MM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data[indexes['train']].values, target_data[indexes['train']].values\n",
    "X_val, y_val = data[indexes['val']].values, target_data[indexes['val']].values\n",
    "X_test, y_test = data[indexes['test']].values, target_data[indexes['test']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого из алгоритмов достигнутое качество должно быть: \n",
    "* RandomForest > 0.84\n",
    "* GradientBoosting > 0.845\n",
    "* AdaBoost > 0.83\n",
    "\n",
    "Обучите каждый из алгоритмов до нужного качества, используйте валидационную выборку, чтобы подбирать гиперпараметры. Получите качество (accuracy) выше необходимого и на validation, и на test.\n",
    "\n",
    "**Подсказка:** для визуализации прогресса обучения можно использовать бибилиотеку [`tqdm`](https://tqdm.github.io/).\n",
    "\n",
    "**Подсказка:** некоторые из подходов анасмблирования тривиальным образом поддаются распараллеливанию на несколько потоков/процессов. Для параллелизации можно использовать `multiprocessing.Pool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 AdaBoost (3 балла)\n",
    "\n",
    "В алгоритме AdaBoost всем объектам обучения присваивается вес `weight`, который определяет степень важности объекта при обучении. И если текущая модель ошибается на некотором объекте, его вес увеличивается, и этот объект будет больше влиять на обучение следующей модели. Также, так как модели обучаются последовательно, они не равносильны между собой, поэтому у каждой модели тоже есть вес `alpha`, который определяет вес модели при суммировании ответов и зависит от количества ошибок `err` модели. На каждой итерации обучения, эти веса пересчитываются по формулам:\n",
    "\n",
    "* $$\\alpha_j = \\log\\left(\\frac{1-err_j}{err_j}\\right),$$\n",
    "где $err_j$ - ошибка классификации\n",
    "\n",
    "* $$w_{new}^t = \\frac{w_{old}^{t}*\\exp(-\\alpha_j \\cdot y(x^t) \\cdot b_j(x^t))}{\\sum\\limits_{i=1}^m w_{old}^{t}*\\exp(-\\alpha_j \\cdot y(x^i) \\cdot b_j(x^i))}$$\n",
    "Изначально все веса объектов $w^i$ равны (и нормированы на 1).\n",
    "\n",
    "Вам предлагается полностью реализовать AdaBoost. Вы можете использовать предыдущие шаблоны, но учтите некоторые пункты:\n",
    "* надо работать с метками {-1,1} - это обусловлено использованием экспоненциальной функции потерь\n",
    "* метод `predict` представляет собой функцию сигмоид, примененную к сумме предсказаний всех моделей  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
