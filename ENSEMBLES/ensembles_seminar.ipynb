{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли\n",
    "\n",
    "### OzonMasters, \"Машинное обучение 1\"\n",
    "\n",
    "В этом ноутбуке вам предлагается реализовать алгоритмы бустинга и бэггинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сэмплирование случайных объектов и признаков\n",
    "\n",
    "Во многих ансамблевых алгоритмах используется прием, заключающийся в обучении на случайной подвыборке объектов или на случайном подмножестве признаков.\n",
    "\n",
    "Так что для начала реализуем класс, который будет упрощать семплирование различных подмассивов данных\n",
    "\n",
    "В классе `ObjectSampler` надо реализовать метод `sample`, который возвращает случайную подвыборку объектов обучения и ответы для них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе `FeaturesSampler` надо реализовать метод `sample`, который возвращает случайную подвыборку индексов признаков, по которым будет производится обучение, а также метод `sample_by_indicies`, который для матрицы объект-признак возвращает подматрицу, в которой содержатся только выбранные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Бэггинг\n",
    "\n",
    "Суть бэггинга заключается в обучении нескольких 'слабых' базовых моделей и объединении их в одну модель, обладающую бОльшей обобщающей способностью. Каждая базовая модель обучается на случайно выбранном подмножестве объектов и на случайно выбранном подмножестве признаков для этих объектов.\n",
    "\n",
    "Ниже вам предлагается реализовать несколько методов класса `Bagger`:\n",
    "* `fit` - обучение базовых моделей\n",
    "* `predict_proba` - вычисление вероятностей ответов\n",
    "* `predict` - вычисление ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Градиентный бустинг\n",
    "\n",
    "Бустинг последовательно обучает набор базовых моделей таким образом, что каждая следующая модель пытается исправить ошибки работы предыдущей модели. Логика того, как учитываются ошибки предыдущей модели может быть разной. В алгоритме градиентного бустинга каждая следующая модель обучается на \"невязках\" предыдущей модели, минимизируя итоговую функцию потерь. У каждого следующего алгоритма вычисляется вес $\\alpha$, с которым он входит в ансамбль. Также есть параметр скорости обучения (learning rate), который не позволяет алгоритму переобучитсья. Вес $\\alpha$ можно находить, используя одномерную оптимизацию. Можно записать процедуру обучения по шагам (будем рассматривать случай бинарной классификации c метками классов {0,1}, чтобы не усложнять жизнь):\n",
    "1. Настройка базового алгоритма $b_0$.\n",
    "    \n",
    "    Алгоритм настраиваются на $y$ с помощью функции MSE.\n",
    "    \n",
    "    \n",
    "2. Будем обозначать текущий небазовый алгоритм - $a$:\n",
    "    \n",
    "    $$a_i(x) = \\sum_{j=0}^i \\alpha_j b_j(x) $$\n",
    "    \n",
    "3. Настройка базового алгоритма $b_i$ (обычно это регрессионное дерево):\n",
    "    \n",
    "    $$b_i = \\arg \\min_b \\sum_{j=1}^l (b(x_j) + \\nabla L(a_{i-1}(x_j), y))^2,$$\n",
    "    т.е. выход очередного базового алгоритма подстраивается под антиградиент функции потерь\n",
    "    \n",
    "4. Настройка веса базового алгоритма $\\alpha_i$:\n",
    "    \n",
    "    $$\\alpha_i = \\min_{\\alpha > 0} \\sum_{j=1}^l L(a_{i-1} + \\alpha b_i(x_j), y) $$\n",
    "    \n",
    "В случае классфикации будем использовать логистическую функцию потерь. Немного упростим ее:\n",
    "\n",
    "$$L = -y\\log\\sigma(a) - (1-y)\\log(1 - \\sigma(a)) = -\\log(1 - \\sigma(a)) - y \\log \\frac{\\sigma(a)}{1 - \\sigma(a)},$$\n",
    "где $\\sigma$ - функция сигмоиды. Ответ после очередного базового алгоритма надо прогонять через сигмоиду, т.к. не гарантируется, что ответы будут лежать на [0,1] - в этом особенность базового алгоритма (который является регрессионным).\n",
    "\n",
    "Преобразуем:\n",
    "$$\\log (1 - \\sigma(a)) = \\log \\frac{1}{1 + \\exp(a)} = -\\log(1 + \\exp(a)) $$\n",
    "\n",
    "$$\\log (\\frac{\\sigma(a)}{1 - \\sigma(a)}) = \\log(\\exp(a)) = a $$\n",
    " \n",
    "Таким образом:\n",
    "\n",
    "$$L = -ya + \\log(1 + \\exp(a))$$\n",
    "\n",
    "Тогда будем вычислять градиент как:\n",
    " \n",
    "$$\\nabla L = - y + \\sigma(a)$$\n",
    "\n",
    "Ниже вам предлагается реализовать стратегию обучения базовых классификаторов для `GradientBoostingClassifier`:\n",
    "* `_fit_first_estimator` - обучение первой базовой модели\n",
    "* `_fit_base_estimator` - обучение базовой модели\n",
    "* `_gradient` - расчет градиента функции ошибки\n",
    "* `_loss` - расчет функции ошибки (для одномерной оптимизации)\n",
    "\n",
    "А также метод `predict_proba` для класса `BoosterClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
